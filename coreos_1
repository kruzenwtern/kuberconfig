#cloud-config
ssh_authorized_keys:
    - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDyYvfXh5ds/gmP1ehlR8gdVwmH0GrFNclLjqcraUkuaTSqMCBTjNkb+0kyH721+w7H3QiSOOsiPuwEv7b+VBlMBti81/EY61R2gBndsGB1f5R9YyWq32vMwxK79lJ41iJuM3Nj5RxEMfrSJta1wGWfClRP3PyPsvkes1GtCY7tbPq1S1miSsvbjdVOiwkxygT0GdsbAWnE9ZIZgOJA2cRYoFUjZZ6PC779U4HX61+MkDtJ4WqZSP8/E2j8/CjuPobkv8t8ZXuS1bOYoFSQF9Tox8RPz9hSEFsdOYVqCnxOZQYGlC3hjKBsS2p9RoigP9w1T1QSXyq2NULqP6F6Qs7T kruz@workbishop

users:
  - name: core
    groups:
      - "sudo"
      - "docker"
    ssh_authorized_keys:
    - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDyYvfXh5ds/gmP1ehlR8gdVwmH0GrFNclLjqcraUkuaTSqMCBTjNkb+0kyH721+w7H3QiSOOsiPuwEv7b+VBlMBti81/EY61R2gBndsGB1f5R9YyWq32vMwxK79lJ41iJuM3Nj5RxEMfrSJta1wGWfClRP3PyPsvkes1GtCY7tbPq1S1miSsvbjdVOiwkxygT0GdsbAWnE9ZIZgOJA2cRYoFUjZZ6PC779U4HX61+MkDtJ4WqZSP8/E2j8/CjuPobkv8t8ZXuS1bOYoFSQF9Tox8RPz9hSEFsdOYVqCnxOZQYGlC3hjKBsS2p9RoigP9w1T1QSXyq2NULqP6F6Qs7T kruz@workbishop

hostname: coreos-1
coreos:
  etcd2:
    name: coreos-1
    initial-advertise-peer-urls: http://192.168.253.130:2380
    listen-peer-urls: http://192.168.253.130:2380,http://192.168.253.130:7001
    listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
    advertise-client-urls: http://192.168.253.130:2379
    initial-cluster-token: "coreos-local-cluster"
    initial-cluster: "coreos-1=http://192.168.253.130:2380,coreos-2=http://192.168.253.140:2380,coreos-3=http://192.168.253.138:2380"
    initial-cluster-state: new
  fleet:
    public-ip: 192.168.253.130
    etcd_servers: http://192.168.253.130:2379
  flannel:
    interface: 192.168.253.130
    etcd_endpoints: http://192.168.253.130:2379,http://192.168.253.140:2379,http://192.168.253.138:2379

  units:
    - name: etcd2.service
      command: start
    - name: fleet.service
      command: start
    - name: flanneld.service
      drop-ins:
        - name: 50-network-config.conf
          content: |
            [Service]
            ExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config '{ "Network": "10.2.0.0/16", "Backend": {"Type": "vxlan"} }'
        - name: 40-ExecStartPre-symlink.conf
          content: |
            [Service]
            ExecStartPre=/usr/bin/ln -sf /etc/flannel/options.env /run/flannel/options.env
    - name: docker.service
      drop-ins:
        - name: 40-flannel.conf
          content: |
            [Unit]
            Requires=flanneld.service
            After=flanneld.service

    - name: kubelet.service
      command: start
      content: |
          [Service]
          Environment=KUBELET_VERSION=v1.5.2_coreos.0
          Environment="RKT_OPTS=--uuid-file-save=/var/run/kubelet-pod.uuid \
            --volume var-log,kind=host,source=/var/log \
            --mount volume=var-log,target=/var/log \
            --volume dns,kind=host,source=/etc/resolv.conf \
            --mount volume=dns,target=/etc/resolv.conf"
          ExecStartPre=/usr/bin/mkdir -p /etc/kubernetes/manifests
          ExecStartPre=/usr/bin/mkdir -p /var/log/containers
          ExecStartPre=-/usr/bin/rkt rm --uuid-file=/var/run/kubelet-pod.uuid
          ExecStart=/usr/lib/coreos/kubelet-wrapper \
            --api-servers=http://127.0.0.1:8080 \
            --register-schedulable=false \
            --cni-conf-dir=/etc/kubernetes/cni/net.d \
            --container-runtime=docker \
            --allow-privileged=true \
            --pod-manifest-path=/etc/kubernetes/manifests \
            --hostname-override=192.168.253.130 \
            --cluster_dns=10.3.0.10 \
            --cluster_domain=cluster.local
          ExecStop=-/usr/bin/rkt stop --uuid-file=/var/run/kubelet-pod.uuid
          Restart=always
          RestartSec=10

          [Install]
          WantedBy=multi-user.target

  - path: /etc/kubernetes/manifests/kube-apiserver.yaml
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-apiserver
        namespace: kube-system
      spec:
        hostNetwork: true
        containers:
        - name: kube-apiserver
          image: quay.io/coreos/hyperkube:v1.5.2_coreos.0
          command:
          - /hyperkube
          - apiserver
          - --bind-address=0.0.0.0
          - --etcd-servers=http://192.168.253.130:2379,http://192.168.253.140:2379,http://192.168.253.138:2379
          - --allow-privileged=true
          - --service-cluster-ip-range=10.3.0.0/24
          - --secure-port=443
          - --advertise-address=192.168.253.130
          - --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota
          - --tls-cert-file=/etc/kubernetes/ssl/apiserver.pem
          - --tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem
          - --client-ca-file=/etc/kubernetes/ssl/ca.pem
          - --service-account-key-file=/etc/kubernetes/ssl/apiserver-key.pem
          - --runtime-config=extensions/v1beta1=true,extensions/v1beta1/networkpolicies=true
          - --anonymous-auth=false
          livenessProbe:
            httpGet:
              host: 127.0.0.1
              port: 8080
              path: /healthz
            initialDelaySeconds: 15
            timeoutSeconds: 15
          ports:
          - containerPort: 443
            hostPort: 443
            name: https
          - containerPort: 8080
            hostPort: 8080
            name: local
          volumeMounts:
          - mountPath: /etc/kubernetes/ssl
            name: ssl-certs-kubernetes
            readOnly: true
          - mountPath: /etc/ssl/certs
            name: ssl-certs-host
            readOnly: true
        volumes:
        - hostPath:
            path: /etc/kubernetes/ssl
          name: ssl-certs-kubernetes
        - hostPath:
            path: /usr/share/ca-certificates
          name: ssl-certs-host
  - path: /etc/kubernetes/manifests/kube-proxy.yaml
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-proxy
        namespace: kube-system
      spec:
        hostNetwork: true
        containers:
        - name: kube-proxy
          image: quay.io/coreos/hyperkube:v1.5.2_coreos.0
          command:
          - /hyperkube
          - proxy
          - --master=http://127.0.0.1:8080
          securityContext:
            privileged: true
          volumeMounts:
          - mountPath: /etc/ssl/certs
            name: ssl-certs-host
            readOnly: true
        volumes:
        - hostPath:
            path: /usr/share/ca-certificates
          name: ssl-certs-host
  - path: /etc/kubernetes/manifests/kube-controller-manager.yaml
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-controller-manager
        namespace: kube-system
      spec:
        hostNetwork: true
        containers:
        - name: kube-controller-manager
          image: quay.io/coreos/hyperkube:v1.5.2_coreos.0
          command:
          - /hyperkube
          - controller-manager
          - --master=http://127.0.0.1:8080
          - --leader-elect=true
          - --service-account-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem
          - --root-ca-file=/etc/kubernetes/ssl/ca.pem
          resources:
            requests:
              cpu: 200m
          livenessProbe:
            httpGet:
              host: 127.0.0.1
              path: /healthz
              port: 10252
            initialDelaySeconds: 15
            timeoutSeconds: 15
          volumeMounts:
          - mountPath: /etc/kubernetes/ssl
            name: ssl-certs-kubernetes
            readOnly: true
          - mountPath: /etc/ssl/certs
            name: ssl-certs-host
            readOnly: true
        volumes:
        - hostPath:
            path: /etc/kubernetes/ssl
          name: ssl-certs-kubernetes
        - hostPath:
            path: /usr/share/ca-certificates
          name: ssl-certs-host
  - path: /etc/kubernetes/manifests/kube-scheduler.yaml
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-scheduler
        namespace: kube-system
      spec:
        hostNetwork: true
        containers:
        - name: kube-scheduler
          image: quay.io/coreos/hyperkube:v1.5.2_coreos.0
          command:
          - /hyperkube
          - scheduler
          - --master=http://127.0.0.1:8080
          - --leader-elect=true
          resources:
            requests:
              cpu: 100m
          livenessProbe:
            httpGet:
              host: 127.0.0.1
              path: /healthz
              port: 10251
            initialDelaySeconds: 15
            timeoutSeconds: 15
