#cloud-config
ssh_authorized_keys:
    - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDyYvfXh5ds/gmP1ehlR8gdVwmH0GrFNclLjqcraUkuaTSqMCBTjNkb+0kyH721+w7H3QiSOOsiPuwEv7b+VBlMBti81/EY61R2gBndsGB1f5R9YyWq32vMwxK79lJ41iJuM3Nj5RxEMfrSJta1wGWfClRP3PyPsvkes1GtCY7tbPq1S1miSsvbjdVOiwkxygT0GdsbAWnE9ZIZgOJA2cRYoFUjZZ6PC779U4HX61+MkDtJ4WqZSP8/E2j8/CjuPobkv8t8ZXuS1bOYoFSQF9Tox8RPz9hSEFsdOYVqCnxOZQYGlC3hjKBsS2p9RoigP9w1T1QSXyq2NULqP6F6Qs7T kruz@workbishop

users:
  - name: core
    groups:
      - "sudo"
      - "docker"
    ssh_authorized_keys:
    - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDyYvfXh5ds/gmP1ehlR8gdVwmH0GrFNclLjqcraUkuaTSqMCBTjNkb+0kyH721+w7H3QiSOOsiPuwEv7b+VBlMBti81/EY61R2gBndsGB1f5R9YyWq32vMwxK79lJ41iJuM3Nj5RxEMfrSJta1wGWfClRP3PyPsvkes1GtCY7tbPq1S1miSsvbjdVOiwkxygT0GdsbAWnE9ZIZgOJA2cRYoFUjZZ6PC779U4HX61+MkDtJ4WqZSP8/E2j8/CjuPobkv8t8ZXuS1bOYoFSQF9Tox8RPz9hSEFsdOYVqCnxOZQYGlC3hjKBsS2p9RoigP9w1T1QSXyq2NULqP6F6Qs7T kruz@workbishop

hostname: coreos-2
coreos:
  etcd2:
    name: coreos-2
    initial-advertise-peer-urls: http://192.168.253.140:2380
    listen-peer-urls: http://192.168.253.140:2380,http://192.168.253.140:7001
    listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
    advertise-client-urls: http://192.168.253.140:2379
    initial-cluster-token: "coreos-local-cluster"
    initial-cluster: "coreos-1=http://192.168.253.130:2380,coreos-2=http://192.168.253.140:2380,coreos-3=http://192.168.253.138:2380"
    initial-cluster-state: new
  fleet:
    public-ip: 192.168.253.140
    etcd_servers: http://192.168.253.140:2379
  flannel:
    interface: 192.168.253.140
    etcd_endpoints: http://192.168.253.130:2379,http://192.168.253.140:2379,http://192.168.253.138:2379

  units:
    - name: etcd2.service
      command: start
    - name: fleet.service
      command: start
    - name: flanneld.service
      drop-ins:
        - name: 50-network-config.conf
          content: |
            [Service]
            ExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config '{ "Network": "10.2.0.0/16", "Backend": {"Type": "vxlan"} }'
        - name: 40-ExecStartPre-symlink.conf
          content: |
            [Service]
            ExecStartPre=/usr/bin/ln -sf /etc/flannel/options.env /run/flannel/options.env
      command: start
    - name: docker.service
      drop-ins:
        - name: 40-flannel.conf
          content: |
            [Unit]
            Requires=flanneld.service
            After=flanneld.service
            [Service]
            EnvironmentFile=/etc/kubernetes/cni/docker_opts_cni.env
    - name: kubelet.service
      command: start
      content: |
        [Service]
        ExecStartPre=/usr/bin/mkdir -p /etc/kubernetes/manifests
        ExecStartPre=/usr/bin/mkdir -p /var/log/containers
        ExecStartPre=-/usr/bin/rkt rm --uuid-file=/var/run/kubelet-pod.uuid
        Environment=KUBELET_VERSION=v1.5.2_coreos.0
        Environment=KUBELET_ACI=quay.io/coreos/hyperkube
        Environment="RKT_OPTS=\
        --volume var-log,kind=host,source=/var/log \
        --mount volume=var-log,target=/var/log \
        --volume dns,kind=host,source=/etc/resolv.conf \
        --mount volume=dns,target=/etc/resolv.conf"

        ExecStart=/usr/lib/coreos/kubelet-wrapper \
        --api-servers=https://192.168.253.130 \
        --network-plugin-dir=/etc/kubernetes/cni/net.d \
        --network-plugin=cni \
        --container-runtime=docker \
        --register-node=true \
        --allow-privileged=true \
        --pod-manifest-path=/etc/kubernetes/manifests \
        --hostname-override=192.168.253.140 \
        --cluster-dns=10.3.0.10 \
        --cluster-domain=cluster.local
        --kubeconfig=/etc/kubernetes/worker-kubeconfig.yaml \
        --tls-cert-file=/etc/kubernetes/ssl/worker.pem \
        --tls-private-key-file=/etc/kubernetes/ssl/worker-key.pem
        ExecStop=-/usr/bin/rkt stop --uuid-file=/var/run/kubelet-pod.uuid
        Restart=always
        RestartSec=10
        [Install]
        WantedBy=multi-user.target

write_files:
  - path: /etc/kubernetes/cni/docker_opts_cni.env
    content: |
      DOCKER_OPT_BIP=""
      DOCKER_OPT_IPMASQ=""
  - path: /etc/kubernetes/cni/net.d/10-flannel.conf
    content: |
      {
        "name": "podnet",
        "type": "flannel",
        "delegate": {
          "isDefaultGateway": true
        }
      }
  - path: /etc/kubernetes/manifests/kube-proxy.yaml
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-proxy
        namespace: kube-system
      spec:
        hostNetwork: true
        containers:
        - name: kube-proxy
          image: quay.io/coreos/hyperkube:v1.5.2_coreos.0
          command:
          - /hyperkube
          - proxy
          - --master=192.168.253.130
          - --kubeconfig=/etc/kubernetes/worker-kubeconfig.yaml
          securityContext:
            privileged: true
          volumeMounts:
          - mountPath: /etc/ssl/certs
            name: ssl-certs
          - mountPath: /etc/kubernetes/worker-kubeconfig.yaml
            name: "kubeconfig"
            readOnly: true
          - mountPath: /etc/kubernetes/ssl
            name: "etc-kube-ssl"
            readOnly: true
        volumes:
        - name: "ssl-certs"
          hostPath:
            path: /usr/share/ca-certificates
        - name: "kubeconfig"
          hostPath:
            path: "/etc/kubernetes/worker-kubeconfig.yaml"
        - name: "etc-kube-ssl"
          hostPath:
            path: "/etc/kubernetes/ssl"
  - path: /etc/kubernetes/worker-kubeconfig.yaml
    content: |
      apiVersion: v1
      kind: Config
      clusters:
      - name: local
        cluster:
          certificate-authority: /etc/kubernetes/ssl/ca.pem
      users:
      - name: kubelet
        user:
          client-certificate: /etc/kubernetes/ssl/worker.pem
          client-key: /etc/kubernetes/ssl/worker-key.pem
      contexts:
      - context:
          cluster: local
          user: kubelet
        name: kubelet-context
      current-context: kubelet-context
